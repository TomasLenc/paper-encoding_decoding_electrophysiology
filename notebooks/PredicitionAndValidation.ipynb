{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'sklearn.model_selection'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-aad4bd150b54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodelingtools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_curve\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlearning_curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'sklearn.model_selection'"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import cross_validation as cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from modelingtools import (delay_time_series, plot_cv_indices,\n",
    "                           cross_validate_alpha)\n",
    "import modelingtools\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import scale, StandardScaler\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, r2_score\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "from itertools import product\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction and Validation\n",
    "In the final tutorial, we will cover prediction and validation with encoding and decoding models. Ultimately, the point of a predictive model is to be able to make predictions about data is hasn't been trained on. Here will cover a few techniques that are useful for doing so.\n",
    "\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_path = '../features/'\n",
    "raw_path = '../raw_data/'\n",
    "sfreq_new = 100  # Cut sfreq in half to save computation time\n",
    "tmin, tmax = -.5, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and visualize the raw ecog data\n",
    "First, we'll load our raw ECoG data to visualize the evoked response on each trial. We'll also resample the data to save some space. Cross-validation can take a long time (though tools like paralellization speed things up considerably, we won't use them here for sake of simplicity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecog = mne.read_epochs(feature_path + 'hfa_ecog-epo.fif', preload=True)\n",
    "ecog.resample(100)\n",
    "ecog.crop(tmin, tmax)\n",
    "\n",
    "_ = ecog.plot(scalings='auto', n_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we'll calculate the activity in each channel and then choose the one with the largest response to speech stimuli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity = ecog.copy()\n",
    "_ = mne.baseline.rescale(activity._data, activity.times, (None, 0),\n",
    "                         mode='zscore', copy=False)\n",
    "activity = activity._data.mean(0).mean(-1)\n",
    "use_elec = np.argmax(activity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the spectrogram feature representation\n",
    "In this section, we'll focus on fitting an encoding model using a spectrogram as inputs. Here we'll load the spectrogram, resample it, and created a delayed representation of it. See the [Model Fitting](./Fitting%20Models.ipynb) notebook for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "spec = mne.read_epochs(feature_path + 'spectrogram-epo.fif')\n",
    "\n",
    "# Take the log so it's more normally-distributed in each frequency band\n",
    "spec._data = np.log(spec._data)\n",
    "\n",
    "# Resample to save space\n",
    "spec.resample(sfreq_new)\n",
    "spec.crop(tmin, tmax)\n",
    "\n",
    "# Create a delayed representation\n",
    "delays = np.linspace(-.4, 0, 20)\n",
    "X_delayed = delay_time_series(spec._data, delays, spec.info['sfreq'])\n",
    "X_delayed = X_delayed.reshape(X_delayed.shape[0], -1, X_delayed.shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation\n",
    "Before we dive into modeling with cross-validation, we'll take a second to explain exactly what cross validation is. The point of cross validation is to separate your data into a group used to fit models, and a group used to test the result of that fit. These are called the *training* and *test* sets, respectively.\n",
    "\n",
    "In `scikit-learn`, we use cross validation objects in order to easily return non-overlapping sets of indices that correspond to training and test sets. If we iterate through these objects, each iteration gives us a different split.\n",
    "\n",
    "For example, here is the result of the `ShuffleSplit` cross validation object, which creates random permutations of the data and splits it up into training / test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cv_iterations = 10\n",
    "indices = np.arange(1000)\n",
    "n_inds = indices.shape[0]\n",
    "cv_example = cv.ShuffleSplit(n_inds, n_iter=n_cv_iterations)\n",
    "plot_cv_indices(cv_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On each iteration, a random subset of indices is left out in a \"test set\" for that iteration. But there's a big problem with this in our case:\n",
    "\n",
    "Many machine learning algorithms assume that data are independent and identically distributed (iid). This basically means that the value of one datapoint shouldn't *a priori* tell us anything about the other datapoints. In neuroscience, we know this isn't true. Noise from one channel spills into another, and any timeseries is autocorrelated (correlated with itself in time). As a result, it is important **not** to let datapoints with these correlations be split into training and test sets. They should stay with one another.\n",
    "\n",
    "A good rule of thumb for timeseries analysis is \"datapoints that are near one another in time should stay with one another in the analyses\". One way to do this is with a \"block\" design that keeps datapoints roughly together in time. An easy way to do this is with something called `KFold` cross validation. \n",
    "\n",
    "In `KFold` cross validation, the data is broken up into `K` groups. On each iteration, one group is left out. The data is fit on the remaining groups, and then tested on the held-out group. Above you can see the blocks of data (in black) that are held out on each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cv_iterations = 10\n",
    "indices = np.arange(1000)\n",
    "n_inds = indices.shape[0]\n",
    "cv_example = cv.KFold(n_inds, n_folds=n_cv_iterations)\n",
    "plot_cv_indices(cv_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the datapoints that are near one another are grouped together in each fold. (If we didn't want this to be the case, we could use hte `shuffle` parameter to shuffle the datapoints before grouping).\n",
    "\n",
    "However, in this case we will still split up *some* datapoints that are near one another in time (the datapoints near the boundaries of each fold). A better way to do cross validation is to iterate over stimulus presentations or trials. Assuming that there is some time between each trial, we are less likely to include dependencies between our training and test sets.\n",
    "\n",
    "For example, below we will show the results of three cross validation routines. Instead of a range of indices that correspond to each datapoint, we'll perform this over a range of indices corresponding to each *trial*. Because these trials happen relatively far from one another in time, now we may do things like shuffling. Let's see what these three iterators looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating across trials (labels)\n",
    "trials = np.arange(len(ecog))\n",
    "cv_iterators = [cv.KFold(len(trials), n_folds=n_cv_iterations),\n",
    "                cv.KFold(len(trials), n_folds=n_cv_iterations, shuffle=True),\n",
    "                cv.LabelShuffleSplit(trials, n_iter=n_cv_iterations,\n",
    "                                     test_size=.1)]\n",
    "\n",
    "fig, axs = plt.subplots(1, len(cv_iterators), figsize=(15, 5),\n",
    "                        sharey=True, sharex=True)\n",
    "for ax, icv in zip(axs, cv_iterators):\n",
    "    ax = plot_cv_indices(icv, ax)\n",
    "    ax.set_title(type(icv).__name__, fontsize=18)\n",
    "fig.suptitle('Cross validation over trials (black = test set)',\n",
    "             fontsize=20, y=1.05)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these cross validation iterators behaves a little differently. The first blocks continuous trials together. The second shuffles the trial numbers then blocks them together. The third simply takes a random sample of trials on each iteration (meaning that some trials might be in the test set on multiple iterations).\n",
    "\n",
    "## Using cross validation to fit our model\n",
    "Now, let's use these cross validation routines to fit our model. We will iterate through one cross validation object, fit a model on a subset of trials, and then use that model to predict the high-frequency activity on the test set of trials. We'll plot the true (in grey) and the predicted (in red) high-frequency activity for each iteration, along with the score on the held-out test data for that iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Times to use in fitting\n",
    "mask_time = mne.utils._time_mask(ecog.times, -.2, 4)\n",
    "\n",
    "# We'll create the delays for the spectrogram ahead of time\n",
    "X_delayed = delay_time_series(spec._data, delays, spec.info['sfreq'])\n",
    "X = X_delayed.reshape(X_delayed.shape[0], -1, X_delayed.shape[-1])\n",
    "X = X[..., mask_time]\n",
    "y = ecog._data[:, use_elec, :][..., mask_time]\n",
    "\n",
    "# We'll use the KFold iterator, shuffling trial numbers first\n",
    "cross_val_iterator = cv.KFold(len(trials), n_folds=5, shuffle=True)\n",
    "model = Ridge(alpha=1e5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize how this cross-validation object will index our data on each iteration:\n",
    "\n",
    "* Each column is an iteration of the outer-loop of cross validation\n",
    "* Red points represent testing data\n",
    "* Blue points represent training data\n",
    "* Shades of blue represent the validation data for each inner-loop (e.g. used to tune hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 8))\n",
    "y_plt = scale(y)\n",
    "for ii, (tr, tt) in enumerate(cross_val_iterator):\n",
    "    # Here we'll define the inner-loop cross validator, which would be used to tune hyperparamters\n",
    "    # For each inner loop iteration, we'll shade the validation set slightly lighter\n",
    "    cv_inner = cv.KFold(len(tr), n_folds=4, shuffle=True)\n",
    "    alphas = np.ones(len(trials))\n",
    "    for (val_tr, val_tt), trans in zip(cv_inner, [.25, .50, .75, 1.]):\n",
    "        alphas[tr[val_tt]] = trans\n",
    "        \n",
    "    # Now create colors according to training / test sets\n",
    "    colors = np.zeros(y.shape[0]).astype(bool)\n",
    "    colors[tt] = True\n",
    "    colors = np.where(colors, 'orangered', 'royalblue')\n",
    "\n",
    "    # Make the plot\n",
    "    for jj, (col, alpha, i_data) in enumerate(zip(colors, alphas, y_plt)):\n",
    "        ixs = np.arange(len(i_data)) + jj * y_plt.shape[-1]\n",
    "        i_plt = i_data + ii * 10\n",
    "        ax.plot(i_plt, ixs, c=col, alpha=alpha)\n",
    "ax.set_xlabel('Outer CV Iterations')\n",
    "ax.set_xticklabels(range(len(cross_val_iterator) + 1))\n",
    "ax.set_title('Training / Test Output Data in Cross Validation')\n",
    "ax.set_ylabel('Time (samples)')\n",
    "ax.text(48, ixs.min(), 'Training Set', color='royalblue',\n",
    "        rotation=-90, fontsize=20, horizontalalignment='left')\n",
    "ax.text(51, ixs.min(), 'Test Set', color='orangered',\n",
    "        rotation=-90, fontsize=20, horizontalalignment='left')\n",
    "ax.axis('tight')\n",
    "_ = plt.setp(ax.get_yticklabels(), visible=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each iteration chooses a different subset of trials for use in the cross-validation. You can try making the same plot with different cross-validation iterators to see how they affect the outcome. (note that here the y-axis is time, matching the shape that scikit-learn expects as inputs)\n",
    "\n",
    "Now, we'll use this cross validation iterator to fit a model, and test it on held-out data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(cross_val_iterator), 1,\n",
    "                        figsize=(10, 5*len(cross_val_iterator)),\n",
    "                        sharex=True)\n",
    "axs[0].set_title('Predicted and Actual High-Frequency Activity')\n",
    "axs[1].set_ylabel('Amplitude (a.u.)')\n",
    "axs[-1].set_xlabel('Time (s)')\n",
    "\n",
    "# Iterate through cross-validation splits\n",
    "for ax, (tr, tt) in zip(axs, cross_val_iterator):\n",
    "    # Pull the training / testing data for the ecog data\n",
    "    y_tr = np.hstack(y[tr]).T    \n",
    "    y_tt = np.hstack(y[tt]).T\n",
    "\n",
    "    # Pull the training / testing data for the spectrogram\n",
    "    X_tr = np.hstack(X[tr]).T\n",
    "    X_tt = np.hstack(X[tt]).T\n",
    "    \n",
    "    # Scale all the features for simplicity\n",
    "    X_tr = scale(X_tr)\n",
    "    X_tt = scale(X_tt)\n",
    "    y_tr = scale(y_tr)\n",
    "    y_tt = scale(y_tt)\n",
    "    \n",
    "    # Fit the model, and use it to predict on new data\n",
    "    model.fit(X_tr, y_tr)\n",
    "    predictions = model.predict(X_tt)\n",
    "    \n",
    "    # Plot the predicted and actual values.\n",
    "    # We'll subtract the mean from each so it's easier to visualize\n",
    "    ax.plot(scale(y_tt), color='k', alpha=.2, lw=2)\n",
    "    ax.plot(scale(predictions), color='r', lw=2)\n",
    "    ax.axis('tight')\n",
    "    \n",
    "    # Finally, plot the coefficient of determination (R2)\n",
    "    print(r2_score(y_tt, predictions))\n",
    "\n",
    "_ = axs[0].legend(['Actual', 'Predicted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the predictions are never perfect, but they do seem to capture some degree of the variability of the true recorded ECoG signal. We can also repeat this procedure for multiple electrodes. We'll only do a single cross-validation loop for each electrode to save time. By visualizing the prediction score for each electrode we get a feel for which regions of the brain are well-modeled by spectro-temporal features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case, we'll keep all the electrodes\n",
    "y_map = ecog._data[..., mask_time]\n",
    "tr, tt = next(iter(cross_val_iterator))\n",
    "\n",
    "# Pull the training / testing data for the ecog data\n",
    "y_tr = np.hstack(y_map[tr]).T    \n",
    "y_tt = np.hstack(y_map[tt]).T\n",
    "\n",
    "# Pull the training / testing data for the spectrogram\n",
    "X_tr = np.hstack(X[tr]).T\n",
    "X_tt = np.hstack(X[tt]).T\n",
    "\n",
    "# Scale all the features for simplicity\n",
    "X_tr = scale(X_tr)\n",
    "X_tt = scale(X_tt)\n",
    "y_tr = scale(y_tr)\n",
    "y_tt = scale(y_tt)\n",
    "\n",
    "# Fit the model, and use it to predict on new data\n",
    "model.fit(X_tr, y_tr)\n",
    "predictions = model.predict(X_tt)\n",
    "scores = r2_score(y_tt, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for y_true, y_pred in zip(y_tt.T, predictions.T):\n",
    "    scores.append(r2_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = plt.imread(raw_path + 'brain.png')\n",
    "melec = pd.read_csv(raw_path + 'meta_elec.csv')\n",
    "xy = melec[['x_2d', 'y_2d']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = modelingtools.plot_activity_on_brain(scores, im, xy[:, 0], xy[:, 1],\n",
    "                                          size_scale=3000, vmin=-.1, vmax=.1,\n",
    "                                          cmap=modelingtools.cmap_score)\n",
    "ax.figure.set_size_inches(10, 10)\n",
    "ax.set_title('Prediction Scores ($R^2$)', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using cross-validation to fit a hyperparameter\n",
    "You may have noticed that throughout these tutorials we have used a relatively arbitrary regularization parameter when fitting models. It is possible to use a more rigorous method for finding the correct parameter to use. This entails an extra cross-validation loop within the training set of data (called an *inner loop*).\n",
    "\n",
    "Below, we'll use an inner loop to fit a model with a range of ridge parameters, and we can see the effect that this has on the model scores.\n",
    "\n",
    "*Note: This might take a minute or two...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose some log-spaced values for alpha\n",
    "alphas = np.logspace(1, 8, 8)\n",
    "n_cv_outer = 3\n",
    "n_cv_inner = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our outer loop iterator\n",
    "outer_iterator = cv.KFold(len(trials), n_folds=n_cv_outer, shuffle=True)\n",
    "scores, coefs = cross_validate_alpha(X, y, outer_iterator, alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On each iteration, we fit and scored the model for a range of alpha parameters. We can take the average score of each alpha parameter, and see how they compare to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the mean score across CV iterations (both inner and outer)\n",
    "mean_scores = scores.mean(0).mean(0)\n",
    "\n",
    "# Plot these values\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.log10(alphas), mean_scores)\n",
    "ix_max = np.argmax(mean_scores)\n",
    "ann_plt = (np.log10(alphas)[ix_max], mean_scores[ix_max])\n",
    "ax.annotate('Chosen $\\lambda$', ann_plt, ann_plt + np.array([0, -.25]),\n",
    "            arrowprops=dict(arrowstyle='->'), fontsize=18)\n",
    "ax.set_xlabel('Alpha (log10)')\n",
    "ax.set_ylabel('Mean score ($R^2$)')\n",
    "ax.set_title('Scores for multiple values of alpha')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is a particular range where the ridge parameter performs best. For small ridge parameters, we get overfitting, for large ridge parameters, we smooth the coefficients by too much. The arrow points to the \"sweet spot\" ridge value that we should use in the rest of our analyses.\n",
    "\n",
    "We can look at the coefficients corresponding to the best alpha parameter to see if they look reasonable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.pcolormesh(delays, range(len(spec.ch_names)),\n",
    "              coefs.mean(0).mean(0)[ix_max, :].reshape([-1, len(delays)]),\n",
    "              cmap=plt.cm.coolwarm, vmin=-.004, vmax=.004)\n",
    "ax.set_yticks(np.arange(len(spec.ch_names))[::5])\n",
    "ax.set_yticklabels(spec.ch_names[::5])\n",
    "ax.axis('tight')\n",
    "ax.set_title('Model Coefficients')\n",
    "ax.set_xlabel('Time Lag (s)')\n",
    "ax.set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: what happens if we don't split by trials?\n",
    "Now we'll see what happens when we split correlated datapoints into the training / test sets. To avoid re-working a bunch of code, we will artificially shuffle the datapoints associated with each trial / time pair. \n",
    "\n",
    "By shuffling these points, it will act as if the data were randomly permuted first before being split into trials. If the datapoints are IID, then this shouldn't make any difference in the outcome of the cross validation. If the data points are not IID (if there are autocorrelations in the data), then there will be some timepoints in the training set that are correlated with timepoints in the test set. The result should be a different selected ridge parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random pairs of trials / timepoints and shuffle all the data\n",
    "# Note that this *shouldn't* change anything if the data is IID (it's not tho)\n",
    "y_bad = y.copy()\n",
    "X_bad = X.copy()\n",
    "ixs = np.array(list(product(range(y_bad.shape[0]), range(y_bad.shape[-1]))))\n",
    "rand_sample = np.random.permutation(range(len(ixs)))\n",
    "ixs_rand = ixs[rand_sample]\n",
    "\n",
    "for (ii_old, jj_old), (ii_new, jj_new) in zip(ixs, ixs_rand):\n",
    "    y_bad[ii_new, jj_new] = y[ii_old, jj_old] \n",
    "    X_bad[ii_new, :, jj_new] = X[ii_old, :, jj_old] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_bad, coefs_bad = cross_validate_alpha(X_bad, y_bad,\n",
    "                                             outer_iterator, alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how the scores are distributed across alpha parameters now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean score across CV iterations for each alpha\n",
    "mean_scores = scores_bad.mean(1).mean(0)\n",
    "\n",
    "# Plot these values\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.log10(alphas), mean_scores)\n",
    "ix_max = np.argmax(mean_scores)\n",
    "ann_plt = (np.log10(alphas)[ix_max], mean_scores[ix_max])\n",
    "ax.annotate('Chosen $\\lambda$', ann_plt, ann_plt + np.array([0, -.25]),\n",
    "            arrowprops=dict(arrowstyle='->'), fontsize=18)\n",
    "ax.set_xlabel('Alpha (log10)')\n",
    "ax.set_ylabel('Mean score ($R^2$)')\n",
    "ax.set_title('Scores for multiple values of alpha')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the cross-validation scores are now best for the lower alpha values. This is because the model picks up on the correlations between the training / test sets, and overfits to these correlations rather than to the true underlying relationships between the datasets. Let's look at the coefficients averaged across CVs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.pcolormesh(delays, range(len(spec.ch_names)),\n",
    "              coefs_bad.mean(0).mean(0)[ix_max, :].reshape([-1, len(delays)]),\n",
    "              cmap=plt.cm.coolwarm)\n",
    "ax.set_yticks(np.arange(len(spec.ch_names))[::5])\n",
    "ax.set_yticklabels(spec.ch_names[::5])\n",
    "ax.axis('tight')\n",
    "ax.set_title('Overfit Coefficients')\n",
    "ax.set_xlabel('Time Lag (s)')\n",
    "ax.set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't any discernible structure to these coefficients, even though the model was doing a pretty good job of predicting on the test set. Any time you see a well-performing model, but the coefficients look indecipherable, there's a good chance that something fishy is going on. Be on the lookout for these kinds of problems when performing cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Prediction / Validation in Decoding\n",
    "For decoding, we will focus on the problem of classification. This is because prediction / validation in decoding models is very similar to the same process in encoding models. Classification poses a different challenge than regression, because outputs cannot be scored using metrics like regression. A classification model is not scored on its ability to explain variance in a test set, but to accurately predict the correct class. As such, the metrics for scoring, and the nature of a model's predictions, look slightly different.\n",
    "\n",
    "Below we will focus on the problem of classifying vowels from plosives, two phoneme types that have very different spectral features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull ECoG phoneme data\n",
    "phonemes = mne.read_epochs(feature_path + 'ecog_phonemes-epo.fif')\n",
    "phoneme_labels = pd.read_csv(raw_path + 'phoneme_labels.csv', index_col=0)\n",
    "phoneme_labels = phoneme_labels.query('phoneme in @phonemes.event_id.keys()')\n",
    "\n",
    "# Define our targets\n",
    "target_ph = ['vowels']\n",
    "non_target_ph = ['plosives']\n",
    "phonemes_targets = phoneme_labels.query('kind in @target_ph')['phoneme'].values\n",
    "phonemes_non_targets = phoneme_labels.query('kind in @non_target_ph')['phoneme'].values\n",
    "\n",
    "# Only include the epochs that include one of our phonemes of choice\n",
    "ecog_targets = phonemes[list(phonemes_targets)]._data.mean(-1)\n",
    "ecog_non_targets = phonemes[list(phonemes_non_targets)]._data.mean(-1)\n",
    "\n",
    "# Now binarize the phoneme labels so we can classify\n",
    "labels_targets = np.ones(ecog_targets.shape[0])\n",
    "labels_non_targets = np.zeros(ecog_non_targets.shape[0])\n",
    "\n",
    "X = np.vstack([ecog_targets, ecog_non_targets])\n",
    "y = np.hstack([labels_targets, labels_non_targets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we'll use a *stratified* cross validation iterator. This is an iterator which ensures that the ratio of classes is the same in each CV iteration. This is important if there are imbalanced numbers of samples in each class (think of an extreme case where there are 3 samples of type A, and 100 samples of type B...most iterations would have no examples of type A, so this accounts for that).\n",
    "\n",
    "We will use a non-linear model for classifying called a Random Forest. These are much harder to interpret, but are generally both faster and more powerful for doing well on testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our CV and model\n",
    "cv_classify = cv.StratifiedShuffleSplit(y, n_iter=100, test_size=.1)\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# This is the ratio of targets to non targets\n",
    "ratio_targets = float(np.sum(y)) / y.shape[0]\n",
    "print(ratio_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through CV iterations\n",
    "# fit the model and predict the class of testing data\n",
    "all_predictions = []\n",
    "all_true = []\n",
    "for tr, tt in cv_classify:\n",
    "    y_tr = y[tr]\n",
    "    y_tt = y[tt]\n",
    "    \n",
    "    X_tr = X[tr]\n",
    "    X_tt = X[tt]\n",
    "    \n",
    "    # Only scale X, since y is a label\n",
    "    X_tr = scale(X_tr)\n",
    "    X_tt = scale(X_tt)\n",
    "    \n",
    "    model.fit(X_tr, y_tr)\n",
    "    predictions = model.predict_proba(X_tt)\n",
    "    all_predictions.append(predictions)\n",
    "    all_true.append(y_tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a collection of predictions for each cross validation iteration. For each set of predictions, we can calculate its Area Under the Curve. We'll use a scikit-learn helper function to do this automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "fpr, tpr, thresholds = roc_curve(all_true[0], all_predictions[0][:, 0],\n",
    "                                 pos_label=1)\n",
    "ax.plot(fpr, tpr, color='k')\n",
    "ax.plot((0, 1), (0, 1), transform=ax.transAxes, ls='--', color='r')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curve for one iteration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this iteration didn't do very well. In an ideal case, the True Positive Rate will be much larger than the False Positive Rate. Here we see that the opposite is true. However, classification can be quite noisy, especially with limited data points and non-linear classifiers. Let's look at how all of our cross-validated performance looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "results = []\n",
    "scores = []\n",
    "scores_chance = []\n",
    "for i_true, i_pred in zip(all_true, all_predictions):\n",
    "    fpr, tpr, thresholds = roc_curve(i_true, i_pred[:, 0], pos_label=1)\n",
    "    score = roc_auc_score(i_true, i_pred[:, 0])\n",
    "    score_chance = roc_auc_score(i_true, np.repeat(ratio_targets, len(i_true)))\n",
    "    df = pd.DataFrame.from_dict({'fpr': fpr, 'tpr': tpr, 'thresh': thresholds})\n",
    "    results.append(df)\n",
    "    scores.append(score)\n",
    "    scores_chance.append(score_chance)\n",
    "    ax.plot(fpr, tpr, alpha=.05, color='k', lw=2)\n",
    "results = pd.concat(results)\n",
    "results['thresh_rnd'] = results['thresh'].round(2)\n",
    "av_results = results.groupby('thresh_rnd').mean().sort_values('fpr')\n",
    "av_results.plot('fpr', 'tpr', ax=ax, color='r', lw=4, label='average ROC')\n",
    "ax.plot((0, 1), (0, 1), ls='--', color='k', lw=3)\n",
    "ax.set_xticks([0, .5, 1])\n",
    "ax.set_yticks([0, .5, 1])\n",
    "_ = plt.setp(ax.get_xticklabels() + ax.get_yticklabels() +\n",
    "             [ax.xaxis.label, ax.yaxis.label], fontsize=18)\n",
    "ax.get_legend().set_visible(False)\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('All ROC Curves', fontsize=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this looks messy. Maybe the curve is above the 50/50 line, maybe not. To determine this we would need to calculate the area under the curve for each iteration, and see if it differed significantly from chance levels.\n",
    "\n",
    "We'll give this a shot by bootstrapping confidence intervals around the mean of the AUC values. For each bootstrap iteration, we'll take a random sample of AUC scores, and calculate its mean. We'll repeat this 1000 times, and then calculate the .5th and 99.5th percentiles of this distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the confidence intervals w/ a bootstrap\n",
    "random_means = [np.mean(np.random.choice(scores, len(scores)))\n",
    "                for _ in range(1000)]\n",
    "lo, hi = np.percentile(random_means, [.5, 99.5])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "_ = ax.axvline(.5, ls='--', color='r', lw=2)\n",
    "_ = ax.hist(scores, color='k')\n",
    "_ = ax.hlines(25, lo, hi, lw=4)\n",
    "ax.set_ylim([0, 30])\n",
    "ax.set_title('AUC Scores for all CVs\\n99% Confidence Interval Above')\n",
    "ax.set_xlabel('AUC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see it's pretty close...the confidence interval is *just* at chance level, and would probably overlap with it at some point given enough repetitions of the above process. Is that a \"significant\" result? There's no single answer to this question, but especially for results that are on the edge of significance, there should be a compelling reason for treating it as such.\n",
    "\n",
    "## Confusion matrices\n",
    "Finally, if we are performing multi-class classification, then it is common to create a *confusion matrix* in order to visualize how classes are confused with one another in the model. `scikit-learn` also has a convenient way for doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some variables that let us define targets for each phoneme\n",
    "rev_event_id = {value: key for key, value in phonemes.event_id.items()}\n",
    "\n",
    "# We'll classify the phoneme class so we have fewer total classes\n",
    "label_mapping = {ph: kind for kind, ph in phoneme_labels.values}\n",
    "\n",
    "# These are integers, one for each class\n",
    "targets = np.array([rev_event_id[ii] for ii in phonemes.events[:, -1]])\n",
    "targets = np.array([label_mapping.get(ii, None) for ii in targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean electrode activity just after each phoneme as input\n",
    "X = phonemes._data.mean(-1)\n",
    "\n",
    "# Remove phonemes we don't want\n",
    "include_targets = ['fricatives', 'nasals', 'plosives', 'vowels']\n",
    "mask_use = [target in include_targets for target in targets.astype(str)]\n",
    "X = X[mask_use]\n",
    "targets = targets[mask_use]\n",
    "df_labels = np.unique(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll fit a classifier for each unique target. It will attempt to classify that target vs. all other classes. It will then validate the model fit in this way by counting the accuracy for all non-targets. As a result, we have a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model + cross validation\n",
    "model = RandomForestClassifier(n_estimators=20)\n",
    "cv = StratifiedShuffleSplit(test_size=0.05, n_splits=10)\n",
    "\n",
    "# Matrices will store the confusion matrix for each CV iteration\n",
    "# It will count false classifications for each phoneme\n",
    "matrices = []\n",
    "for train, test in cv.split(X, targets):\n",
    "    model.fit(X[train], targets[train])\n",
    "    predictions = model.predict(X[test])\n",
    "    matrix = confusion_matrix(targets[test], predictions,\n",
    "                              labels=df_labels)\n",
    "    matrix = matrix / matrix.sum(-1)  # Divide by total \"true\" classes for this iteration to gat a %\n",
    "    matrices.append(matrix)\n",
    "matrix = np.array(matrices).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll show the heatmap\n",
    "matrix = pd.DataFrame(matrix, index=df_labels, columns=df_labels)\n",
    "ax = sns.heatmap(matrix, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, this is actually not a very good result - ideally we'd see a nice diagonal line, indicating that samples were correctly classified as their true class."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": false,
   "toc_threshold": "2",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
